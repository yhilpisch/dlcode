{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9a2f5a",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b46690",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc1df2",
   "metadata": {},
   "source": [
    "# Chapter 14 â€” Recurrent and Attention-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976ab2f",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention (masked vs unmasked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "\n",
    "def attn(Q, K, V, mask=None):\n",
    "    # Scaled dot-product attention\n",
    "    S = (Q @ K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "    if mask is not None:\n",
    "        S = S.masked_fill(~mask, float('-inf'))\n",
    "    A = torch.softmax(S, dim=-1)\n",
    "    return A @ V, A\n",
    "\n",
    "T, d = 4, 3\n",
    "Q = torch.randn(T, d)  # query vectors\n",
    "K = torch.randn(T, d)  # key vectors\n",
    "V = torch.randn(T, d)  # value vectors\n",
    "\n",
    "# Causal mask (lower triangular: prevent attending to future positions)\n",
    "causal = torch.tril(torch.ones(T, T, dtype=torch.bool))\n",
    "attn(Q, K, V, causal)[1].shape, attn(Q, K, V, None)[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73e43b",
   "metadata": {},
   "source": [
    "## Multi-head attention shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d879a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, d_model, h = 2, 5, 8, 2\n",
    "d_head = d_model//h\n",
    "x = torch.randn(B, T, d_model)\n",
    "Wq = Wk = Wv = Wo = torch.randn(d_model, d_model)\n",
    "Q = x@Wq # query vectors  # query vectors\n",
    "K = x@Wk # key vectors  # key vectors\n",
    "V = x@Wv # value vectors  # value vectors\n",
    "Q = Q.view(B, T, h, d_head).transpose(1, 2) # query vectors  # query vectors\n",
    "K = K.view(B, T, h, d_head).transpose(1, 2) # key vectors  # key vectors\n",
    "V = V.view(B, T, h, d_head).transpose(1, 2) # value vectors  # value vectors\n",
    "((Q@K.transpose(-2, -1))/ (d_head**0.5)).shape\n",
    "# torch.Size([2, 2, 5, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c90ec",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n\n",
    "1. Change the number of heads or d_model in a tiny transformer; compare training curves.\n",
    "2. Visualize attention for a few prompts and discuss patterns you see.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
