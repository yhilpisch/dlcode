{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92bc1df2",
      "metadata": {},
      "source": [
        "# Chapter 14 \u2014 Recurrent and Attention-based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8976ab2f",
      "metadata": {},
      "source": [
        "## Scaled dot-product attention (masked vs unmasked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "135b6c1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, math\n",
        "\n",
        "def attn(Q, K, V, mask = None):\n",
        "    # Scaled dot-product attention\n",
        "    S = (Q @ K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
        "    if mask is not None:\n",
        "        S = S.masked_fill(~mask, float('-inf'))\n",
        "    A = torch.softmax(S, dim = -1)\n",
        "    return A @ V, A\n",
        "\n",
        "T, d = 4, 3\n",
        "Q = torch.randn(T, d) # query vectors # query vectors  # query vectors\n",
        "K = torch.randn(T, d) # key vectors # key vectors  # key vectors\n",
        "V = torch.randn(T, d) # value vectors # value vectors  # value vectors\n",
        "\n",
        "# Causal mask (lower triangular: prevent attending to future positions)\n",
        "causal = torch.tril(torch.ones(T, T, \n",
        "    dtype = torch.bool)) # lower-triangular mask (no future attention)\n",
        "attn(Q, K, V, causal)[1].shape, attn(Q, K, V, None)[1].shape # (torch.Size([4, 4]),     torch.Size([4, 4]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b73e43b",
      "metadata": {},
      "source": [
        "## Multi-head attention shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d879a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "B, T, d_model, h = 2, 5, 8, 2\n",
        " d_head = d_model//h\n",
        "x = torch.randn(B, T, d_model)\n",
        "Wq = Wk = Wv = Wo = torch.randn(d_model, d_model)\n",
        "Q = x@Wq # query vectors # query vectors  # query vectors\n",
        " K = x@Wk # key vectors # key vectors  # key vectors\n",
        " V = x@Wv # value vectors # value vectors  # value vectors\n",
        "Q = Q.view(B, T, h, d_head).transpose(1, 2) # query vectors # query vectors  # query vectors\n",
        "K = K.view(B, T, h, d_head).transpose(1, 2) # key vectors # key vectors  # key vectors\n",
        "V = V.view(B, T, h, d_head).transpose(1, 2) # value vectors # value vectors  # value vectors\n",
        "((Q@K.transpose(-2, -1))/ (d_head**0.5)).shape\n",
        "# torch.Size([2, 2, 5, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}