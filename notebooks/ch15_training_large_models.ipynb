{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69497232",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2840a",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a6243",
   "metadata": {},
   "source": [
    "# Chapter 15 — Training Large Models (DDP, AMP, Checkpointing)\n",
    "\n",
    "This notebook provides small, runnable snippets that mirror the chapter’s concepts:\n",
    "\n",
    "- Gradient accumulation (larger effective batch on limited memory)\n",
    "- AMP (automatic mixed precision) demo when CUDA is available\n",
    "- Checkpointing patterns (save/load state)\n",
    "- DDP (Distributed Data Parallel) launch sketch (reference code)\n",
    "\n",
    "These are minimal, CPU-friendly examples intended to build intuition — not full training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2b9a6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n",
    "\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401abefa",
   "metadata": {},
   "source": [
    "### Inspect environment and device\n",
    "\n",
    "Import the libraries we need and report the interpreter and accelerator availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')  # show interpreter version\n",
    "print(f'Torch : {torch.__version__}')  # show torch version\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # prefer GPU when available\n",
    "print('Device:', device)  # confirm compute target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44403189",
   "metadata": {},
   "source": [
    "## Gradient Accumulation (CPU-friendly demo)\n",
    "\n",
    "Accumulate gradients over multiple micro-batches to simulate a larger batch size without increasing memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f72b32",
   "metadata": {},
   "source": [
    "### Simulate gradient accumulation\n",
    "\n",
    "Train a tiny regression model with micro-batch accumulation to emulate a larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9462063",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # reproducibility\n",
    "N, D = 512, 16  # dataset size and feature width\n",
    "X = torch.randn(N, D)  # inputs\n",
    "true_w = torch.randn(D, 1)  # ground-truth weights\n",
    "y = X @ true_w + 0.1 * torch.randn(N, 1)  # noisy labels\n",
    "ds = TensorDataset(X, y)  # wrap tensors as a dataset\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)  # iterate mini-batches\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D, 32), nn.ReLU(), nn.Linear(32, 1)).to(device)  # tiny regression network\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)  # SGD optimizer\n",
    "accum_steps = 4  # number of micro-batches per update\n",
    "loss_fn = nn.MSELoss()  # mean squared error loss\n",
    "\n",
    "model.train()  # enable training mode\n",
    "running = 0.0  # track scaled loss\n",
    "opt.zero_grad(set_to_none=True)  # reset gradients before loop\n",
    "for step, (xb, yb) in enumerate(loader, start=1):\n",
    "    xb, yb = xb.to(device), yb.to(device)  # move data to device\n",
    "    pred = model(xb)  # forward pass\n",
    "    loss = loss_fn(pred, yb) / accum_steps  # scale loss for accumulation\n",
    "    loss.backward()  # backprop scaled loss\n",
    "    running += loss.item()  # accumulate scalar loss\n",
    "    if step % accum_steps == 0:\n",
    "        opt.step()  # apply gradients\n",
    "        opt.zero_grad(set_to_none=True)  # clear gradients for next cycle\n",
    "        print(f'step {step:3d} loss {running:.4f}')  # report running loss\n",
    "        running = 0.0  # reset accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67007290",
   "metadata": {},
   "source": [
    "## AMP (Automatic Mixed Precision)\n",
    "\n",
    "Use `torch.cuda.amp.autocast` and `GradScaler` on CUDA for speed/memory benefits. Skips on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d9c3c",
   "metadata": {},
   "source": [
    "### Demonstrate automatic mixed precision\n",
    "\n",
    "Run a single automatic mixed-precision step when CUDA is available, or explain why it is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c70a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "use_amp = torch.cuda.is_available()  # only run AMP on CUDA\n",
    "if use_amp:\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=True)  # manage scaled gradients\n",
    "    model = nn.Sequential(nn.Linear(D, 128), nn.ReLU(), nn.Linear(128, 1)).to('cuda')  # beefier AMP model\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)  # optimizer for AMP example\n",
    "    model.train()  # enable training mode\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()  # move batch to GPU\n",
    "        opt.zero_grad(set_to_none=True)  # clear gradients\n",
    "        with torch.amp.autocast('cuda', enabled=True):  # mixed-precision region\n",
    "            pred = model(xb)  # forward pass\n",
    "            loss = nn.functional.mse_loss(pred, yb)  # compute loss\n",
    "        scaler.scale(loss).backward()  # backprop scaled loss\n",
    "        scaler.step(opt)  # apply optimizer step\n",
    "        scaler.update()  # adjust scaling factor\n",
    "        break  # single AMP iteration for illustration\n",
    "    message = 'AMP step complete on CUDA'  # success status\n",
    "else:\n",
    "    message = 'CUDA not available; AMP demo skipped.'  # fallback status\n",
    "\n",
    "message  # display outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa35de6",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Save and load model/optimizer state for resumable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5d7c8",
   "metadata": {},
   "source": [
    "### Save and reload checkpoints\n",
    "\n",
    "Store the model and optimizer state, then reload to confirm the checkpoint workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'checkpoint_ch15_demo.pt'  # checkpoint file location\n",
    "state = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': opt.state_dict(),\n",
    "    'meta': {'epoch': 1, 'accum_steps': accum_steps},\n",
    "}\n",
    "torch.save(state, ckpt_path)  # serialize training state\n",
    "print('Saved checkpoint to', ckpt_path)  # confirm save\n",
    "\n",
    "loaded = torch.load(ckpt_path, map_location=device)  # load checkpoint on current device\n",
    "model.load_state_dict(loaded['model'])  # restore model weights\n",
    "opt.load_state_dict(loaded['optimizer'])  # restore optimizer\n",
    "print('Restored epoch:', loaded['meta']['epoch'])  # confirm metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd70b8",
   "metadata": {},
   "source": [
    "## DDP Launch Sketch (reference)\n",
    "\n",
    "Full DDP runs are typically launched via the CLI using `torchrun` (or `python -m torch.distributed.run`). Below is a minimal training script outline. Save as `train_ddp.py` and launch with:\n",
    "\n",
    "```bash\n",
    "torchrun --standalone --nproc_per_node=2 train_ddp.py\n",
    "```\n",
    "This cell shows the code for reference (not executed here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386aa543",
   "metadata": {},
   "source": [
    "### Provide a minimal DDP training script\n",
    "\n",
    "Include a ready-to-run DistributedDataParallel example as a reference string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e31310",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddp_code = r'''\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group('nccl' if torch.cuda.is_available() else 'gloo')\n",
    "    rank = dist.get_rank()\n",
    "    device = rank % torch.cuda.device_count() if torch.cuda.is_available() else 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(device)\n",
    "\n",
    "    # Toy data\n",
    "    X = torch.randn(1024, 16)  # synthetic inputs\n",
    "    y = torch.randn(1024, 1)  # synthetic targets\n",
    "    ds = TensorDataset(X, y)  # wrap tensors as a dataset\n",
    "    sampler = DistributedSampler(ds, shuffle=True) if dist.is_initialized() else None  # shard data across ranks\n",
    "    loader = DataLoader(ds, batch_size=64, sampler=sampler, shuffle=(sampler is None))  # construct loader\n",
    "\n",
    "    model = nn.Sequential(nn.Linear(16, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda(device)\n",
    "    model = DDP(model, device_ids=[device] if torch.cuda.is_available() else None)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)  # optimizer configuration\n",
    "    loss_fn = nn.MSELoss()  # regression loss\n",
    "\n",
    "    for epoch in range(2):\n",
    "        if sampler is not None:\n",
    "            sampler.set_epoch(epoch)\n",
    "        for xb, yb in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                xb, yb = xb.cuda(device), yb.cuda(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(xb), yb)  # forward and loss computation\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if rank == 0:\n",
    "            print('epoch', epoch)\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "    print(ddp_code)  # show reference script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17897b",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Sketch a pseudo-DDP setup: outline processes, seed management, and gradients.\n",
    "2. Compare gradient accumulation vs. true data parallelism conceptually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eb0f4",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
