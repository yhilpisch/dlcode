{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e62964",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2840a",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a6243",
   "metadata": {},
   "source": [
    "# Chapter 15 — Training Large Models (DDP, AMP, Checkpointing)\n",
    "\n",
    "This notebook provides small, runnable snippets that mirror the chapter’s concepts:\n",
    "\n",
    "- Gradient accumulation (larger effective batch on limited memory)\n",
    "- AMP (automatic mixed precision) demo when CUDA is available\n",
    "- Checkpointing patterns (save/load state)\n",
    "- DDP (Distributed Data Parallel) launch sketch (reference code)\n",
    "\n",
    "These are minimal, CPU-friendly examples intended to build intuition — not full training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, sys, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'Torch : {torch.__version__}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44403189",
   "metadata": {},
   "source": [
    "## Gradient Accumulation (CPU-friendly demo)\n",
    "\n",
    "Accumulate gradients over multiple micro-batches to simulate a larger batch size without increasing memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9462063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny synthetic regression problem\n",
    "torch.manual_seed(0)  # reproducibility\n",
    "N, D = 512, 16\n",
    "X = torch.randn(N, D)  # inputs\n",
    "true_w = torch.randn(D, 1)\n",
    "y = X @ true_w + 0.1*torch.randn(N, 1)  # targets/labels\n",
    "ds = TensorDataset(X, y)  # wrap tensors as a dataset\n",
    "loader = DataLoader(ds, batch_size = 32, shuffle = True)  # create data loader\n",
    "\n",
    "model = nn.Sequential(nn.Linear(D, 32), nn.ReLU(), nn.Linear(32, 1)).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr = 0.1)  # optimizer setup / step\n",
    "accum_steps = 4  # 32*4 = effective 128\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "running = 0.0\n",
    "opt.zero_grad(set_to_none = True)\n",
    "for step, (xb, yb) in enumerate(loader, start = 1):\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    pred = model(xb)  # forward pass / predictions\n",
    "    loss = loss_fn(pred, yb) / accum_steps  # training objective\n",
    "    loss.backward()\n",
    "    running += loss.item()\n",
    "    if step % accum_steps == 0:\n",
    "        opt.step()\n",
    "        opt.zero_grad(set_to_none = True)\n",
    "        # report every accumulation to show progress\n",
    "        print(f'step {step:3d} loss {running:.4f}')\n",
    "        running = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67007290",
   "metadata": {},
   "source": [
    "## AMP (Automatic Mixed Precision)\n",
    "\n",
    "Use `torch.cuda.amp.autocast` and `GradScaler` on CUDA for speed/memory benefits. Skips on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c70a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "use_amp = torch.cuda.is_available()\n",
    "if use_amp:\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
    "    model = nn.Sequential(nn.Linear(D, 128), nn.ReLU(), nn.Linear(128, 1)).to('cuda')\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            pred = model(xb)\n",
    "            loss = nn.functional.mse_loss(pred, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            break\n",
    "            print('AMP step complete on CUDA')\n",
    "            else:\n",
    "                print('CUDA not available; AMP demo skipped.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa35de6",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Save and load model/optimizer state for resumable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'checkpoint_ch15_demo.pt'\n",
    "state = {\n",
    "'model': model.state_dict(),\n",
    "'optimizer': opt.state_dict(),\n",
    "'meta': {'epoch': 1, 'accum_steps': accum_steps}\n",
    "}\n",
    "torch.save(state, ckpt_path)\n",
    "print('Saved checkpoint to', ckpt_path)\n",
    "\n",
    "# restore\n",
    "loaded = torch.load(ckpt_path, map_location = device)\n",
    "model.load_state_dict(loaded['model'])\n",
    "opt.load_state_dict(loaded['optimizer'])\n",
    "print('Restored epoch:', loaded['meta']['epoch'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd70b8",
   "metadata": {},
   "source": [
    "## DDP Launch Sketch (reference)\n",
    "\n",
    "Full DDP runs are typically launched via the CLI using `torchrun` (or `python -m torch.distributed.run`). Below is a minimal training script outline. Save as `train_ddp.py` and launch with:\n",
    "\n",
    "```bash\n",
    "torchrun --standalone --nproc_per_node=2 train_ddp.py\n",
    "```\n",
    "This cell shows the code for reference (not executed here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e31310",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddp_code = r'''\n",
    "import os, torch, torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group('nccl' if torch.cuda.is_available() else 'gloo')\n",
    "    rank = dist.get_rank()\n",
    "    device = rank % torch.cuda.device_count() if torch.cuda.is_available() else 'cpu'\n",
    "    if torch.cuda.is_available(): torch.cuda.set_device(device)\n",
    "\n",
    "    # Toy data\n",
    "    X = torch.randn(1024, 16)  # inputs\n",
    "    y = torch.randn(1024, 1)  # targets/labels\n",
    "    ds = TensorDataset(X, y)  # wrap tensors as a dataset\n",
    "    sampler = DistributedSampler(ds,  # shard dataset across ranks\n",
    "        shuffle = True) if dist.is_initialized() else None  # shard dataset across ranks\n",
    "    loader = DataLoader(ds, batch_size = 64, sampler = sampler,  # create data loader\n",
    "        shuffle = (sampler is None))  # create data loader\n",
    "\n",
    "    model = nn.Sequential(nn.Linear(16, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "    if torch.cuda.is_available(): model = model.cuda(device)\n",
    "    model = DDP(model, device_ids = [device] if torch.cuda.is_available() else None)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr = 1e-3)  # optimizer setup / step\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(2):\n",
    "        if sampler is not None: sampler.set_epoch(epoch)\n",
    "        for xb, yb in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                xb, yb = xb.cuda(device), yb.cuda(device)\n",
    "            opt.zero_grad(set_to_none = True)\n",
    "            loss = loss_fn(model(xb), yb)  # training objective\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if rank == 0:\n",
    "            print('epoch', epoch)\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "print(ddp_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbc78c",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}