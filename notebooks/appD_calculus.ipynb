{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix D â€” Calculus Essentials\n\nColab-ready, self-contained notebook with tiny, verifiable examples and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Optional: Colab usually has these\n",
    "# !pip -q install numpy matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8') # plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Differences to Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x*x\n",
    "x0 = 1.0\n",
    "for h in [1e-1, 1e-2, 1e-3, 1e-6]:\n",
    "    slope = (f(x0+h)-f(x0))/h\n",
    "    print(h, slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tangent at x0=1\n",
    "xs = np.linspace(-0.5, 2.5, 400)\n",
    "ys = f(xs)\n",
    "m = 2*x0\n",
    "b = f(x0) - m*x0\n",
    "yt = m*xs + b\n",
    "plt.figure(figsize = (4.8, 3.2)) # plotting\n",
    "plt.plot(xs, ys, label = 'f(x) = x^2', lw = 2) # plotting\n",
    "plt.plot(xs, yt, label = 'tangent', lw = 2) # plotting\n",
    "plt.scatter([x0], [f(x0)], c = 'k', s = 30) # plotting\n",
    "plt.legend(frameon = False) # plotting\n",
    "plt.tight_layout() # plotting\n",
    "plt.show() # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Rules (Chain Rule Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.234\n",
    "lhs = 2*np.sin(x)*np.cos(x)\n",
    "h = 1e-6 # hidden activations  # hidden activations\n",
    "rhs = ((np.sin(x+h))**2 - (np.sin(x))**2)/h\n",
    "round(lhs, 6), round(rhs, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Differences: Forward vs Central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.0\n",
    "true = np.cos(x0)\n",
    "hs = np.logspace(-8, -1, 40)\n",
    "fwd_err = []\n",
    "cen_err = []\n",
    "for h in hs:\n",
    "    fwd = (np.sin(x0 + h) - np.sin(x0)) / h\n",
    "    cen = (np.sin(x0 + h) - np.sin(x0 - h)) / (2 * h)\n",
    "    fwd_err.append(abs(fwd - true))\n",
    "    cen_err.append(abs(cen - true))\n",
    "    plt.figure(figsize = (4.8, 3.2)) # plotting\n",
    "    plt.loglog(hs, fwd_err, label = 'forward (O(h))') # plotting\n",
    "    plt.loglog(hs, cen_err, label = 'central (O(h^2))') # plotting\n",
    "    plt.xlabel('h') # plotting\n",
    "    plt.ylabel('abs error') # plotting\n",
    "    plt.legend(frameon = False) # plotting\n",
    "    plt.grid(True, which = 'both', ls = ':', alpha = 0.4) # plotting\n",
    "    plt.tight_layout() # plotting\n",
    "    plt.show() # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partials and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x, y): return x*x + x*y\n",
    "x, y = 2.0, -1.0\n",
    "dfx, dfy = 2*x + y, x\n",
    "eps = 1e-6\n",
    "dfx_fd = (f2(x+eps, y) - f2(x, y))/eps\n",
    "dfy_fd = (f2(x, y+eps) - f2(x, y))/eps\n",
    "np.allclose([dfx, dfy], [dfx_fd, dfy_fd], rtol = 1e-6, atol = 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Gradient Descent (2D Quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fquad(x, y): return x*x + 0.5*y*y\n",
    "def g(x, y): return np.array([2*x, y])\n",
    "xy = np.array([-3.0, 2.0])\n",
    "eta = 0.2\n",
    "vals = []\n",
    "for t in range(10):\n",
    "    xy = xy - eta*g(*xy)\n",
    "    vals.append(fquad(*xy))\n",
    "    vals[:3], vals[-3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contours and GD path\n",
    "gx = np.linspace(-3.5, 0.5, 200)\n",
    "gy = np.linspace(-2.5, 2.5, 200)\n",
    "XX, YY = np.meshgrid(gx, gy)\n",
    "ZZ = fquad(XX, YY)\n",
    "xy = np.array([-3.0, 2.0])\n",
    "eta = 0.2\n",
    "pts = [xy.copy()]\n",
    "for _ in range(18): xy = xy - eta*g(*xy)\n",
    "pts.append(xy.copy())\n",
    "pts = np.array(pts)\n",
    "plt.figure(figsize = (4.8, 3.6)) # plotting\n",
    "cs = plt.contour(XX, YY, ZZ, levels = 12, cmap = 'Greys',   # plotting\n",
    "alpha = 0.8) # plotting\n",
    "plt.clabel(cs, inline = True, fontsize = 8, fmt = '%.1f') # plotting\n",
    "plt.plot(pts[:, 0], pts[:, 1], 'o-', ms = 3.5, lw = 1.2,   # plotting\n",
    "label = 'GD path') # plotting\n",
    "plt.scatter([0], [0], c = 'k', s = 20, label = 'min') # plotting\n",
    "plt.legend(frameon = False) # plotting\n",
    "plt.tight_layout() # plotting\n",
    "plt.show() # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "appD_calculus.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}