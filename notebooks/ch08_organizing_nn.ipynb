{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71e0c7c",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n",
        "\n",
        "**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8722545",
      "metadata": {},
      "source": [
        "# Chapter 8 â€” Organizing Code with torch.nn\n",
        "Refactor the tiny MLP with nn.Module; add train/eval and checkpointing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n\n",
        "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
        "Use it as a companion to the chapter: run each cell, read the short notes,\n",
        "and try small variations to build intuition.\n\n",
        "Tips:\n",
        "- Run cells top to bottom; restart kernel if state gets confusing.\n",
        "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
        "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d97e56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install torch numpy matplotlib scikit-learn\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "plt.style.use('seaborn-v0_8')  # plotting\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a5fced",
      "metadata": {},
      "source": [
        "## Define model and training helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9a301f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, in_dim=2, hidden=16, out_dim=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "  # Prepare data (moons)\n",
        "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
        "X_te = torch.tensor(X_te, dtype=torch.float32)\n",
        "y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
        "y_te = torch.tensor(y_te, dtype=torch.long)\n",
        "\n",
        "  # Model, optimizer, loss\n",
        "model = TinyMLP()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def accuracy(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a366f90",
      "metadata": {},
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dfd30a",
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = []\n",
        "acc_history = []\n",
        "for _ in range(50):\n",
        "    model.train()\n",
        "    logits = model(X_tr)  # raw model scores before softmax/sigmoid\n",
        "    loss = loss_fn(logits, y_tr)  # training objective\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    losses.append(float(loss.detach()))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        acc = accuracy(model(X_te), y_te)\n",
        "    acc_history.append(acc)\n",
        "\n",
        "losses[-1], acc_history[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, matplotlib.pyplot as plt  # NumPy for grid; Matplotlib for plotting\n",
        "# Concatenate train and test to get plotting ranges\n",
        "allX = torch.cat([X_tr, X_te], dim=0).numpy()  # (N, 2) array for min/max\n",
        "# Build a dense grid over feature space\n",
        "xx, yy = np.meshgrid(  # coordinate matrices for contour plot\n",
        "    np.linspace(allX[:,0].min()-1, allX[:,0].max()+1, 300),  # x range\n",
        "    np.linspace(allX[:,1].min()-1, allX[:,1].max()+1, 300)   # y range\n",
        ")\n",
        "# Stack grid coordinates into a (M, 2) tensor\n",
        "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)  # evaluation points\n",
        "model.eval()  # switch to eval mode (no dropout/batchnorm effects)\n",
        "with torch.no_grad():  # disable gradient tracking for inference\n",
        "    zz = model(grid).argmax(1).reshape(xx.shape).numpy()  # class index per grid point\n",
        "# Plot decision regions and test points\n",
        "plt.figure(figsize=(5.2, 3.6))  # width x height in inches\n",
        "plt.contourf(xx, yy, zz, alpha=0.25, cmap='coolwarm')  # filled contour of classes\n",
        "plt.scatter(X_te[:,0], X_te[:,1], c=y_te, s=12, edgecolor='k')  # test points overlay\n",
        "plt.title('Decision boundary (TinyMLP)')  # figure title\n",
        "plt.tight_layout(); plt.show()  # tidy layout and render\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # plotting backend\n",
        "plt.figure(figsize=(5.5, 2.8))  # set compact figure size\n",
        "plt.plot(losses, label='train loss')  # line plot of training loss per epoch\n",
        "plt.plot(acc_history, label='test acc', linestyle='--')  # dashed accuracy curve\n",
        "plt.xlabel('epoch'); plt.ylabel('value')  # axis labels\n",
        "plt.legend(frameon=False)  # simple legend without frame\n",
        "plt.tight_layout(); plt.show()  # tighten layout and render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n",
        "1. Refactor a model into a clean nn.Module; add a `forward` docstring.\n",
        "2. Add a small evaluation helper that reports accuracy/loss concisely.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}