{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 10 \u2014 Improving Training\n",
        "Regularization, early stopping, and LR schedules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install torch numpy matplotlib scikit-learn\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "plt.style.use('seaborn-v0_8') # plotting # plotting  # plotting\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/val curves with/without weight decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def curves_l2(weight_decay):\n",
        "    torch.manual_seed(0) # reproducibility\n",
        "    X, y = make_moons(n_samples = 1000, noise = 0.35, random_state = 0)\n",
        "    # small train split to induce overfitting and highlight L2\n",
        "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size = 0.65,         random_state = 42, stratify = y)\n",
        "    X_va, _, y_va, _ = train_test_split(X_tmp, y_tmp, test_size = 0.65,         random_state = 42, stratify = y_tmp)\n",
        "    X_tr = torch.tensor(X_tr, dtype = torch.float32)\n",
        " y_tr = torch.tensor(y_tr, dtype = torch.long)\n",
        "    X_va = torch.tensor(X_va, dtype = torch.float32)\n",
        " y_va = torch.tensor(y_va, dtype = torch.long)\n",
        "    model = nn.Sequential(nn.Linear(2, 256), nn.ReLU(), nn.Linear(256, 2))\n",
        "    opt = torch.optim.Adam(model.parameters(), lr = 5e-3, \n",
        "        weight_decay = weight_decay) # optimizer setup / step\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        " tr, va = [], []\n",
        "    for _ in range(150):\n",
        "        model.train()\n",
        " logits = model(X_tr) # raw model scores before softmax/sigmoid\n",
        " loss = loss_fn(logits, y_tr) # training objective\n",
        "        opt.zero_grad()\n",
        " loss.backward()\n",
        " opt.step()\n",
        "        model.eval();\n",
        "        with torch.no_grad():\n",
        "            tr.append(float(loss))\n",
        " va.append(float(loss_fn(model(X_va), y_va)))\n",
        "    return tr, va\n",
        "tr0, va0 = curves_l2(0.0)\n",
        "tr1, va1 = curves_l2(5e-3)\n",
        "e = range(1, len(tr0)+1)\n",
        "plt.figure(figsize = (7.6, 3.0)) # plotting # plotting  # plotting\n",
        "plt.subplot(1, 2, 1) # plotting # plotting  # plotting\n",
        " plt.plot(e, tr0, label = 'train') # plotting # plotting  # plotting\n",
        " plt.plot(e, va0, label = 'val') # plotting # plotting  # plotting\n",
        "plt.title('no weight decay') # plotting # plotting  # plotting\n",
        " plt.xlabel('epoch') # plotting # plotting  # plotting\n",
        " plt.ylabel('loss') # plotting # plotting  # plotting\n",
        " plt.legend(frameon = False) # plotting # plotting  # plotting\n",
        "plt.subplot(1, 2, 2) # plotting # plotting  # plotting\n",
        " plt.plot(e, tr1, label = 'train') # plotting # plotting  # plotting\n",
        " plt.plot(e, va1, label = 'val') # plotting # plotting  # plotting\n",
        "plt.title('weight decay 0.005') # plotting # plotting  # plotting\n",
        " plt.xlabel('epoch') # plotting # plotting  # plotting\n",
        " plt.legend(frameon = False) # plotting # plotting  # plotting\n",
        "plt.tight_layout() # plotting # plotting  # plotting\n",
        " plt.show() # plotting # plotting  # plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropout p=0.0 vs p=0.6 \u2014 validation curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def curves_dropout(p):\n",
        "    torch.manual_seed(0) # reproducibility\n",
        "    X, y = make_moons(n_samples = 1000, noise = 0.40, random_state = 0)\n",
        "    # even smaller train split to surface regularization effect\n",
        "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size = 0.7,         random_state = 42, stratify = y)\n",
        "    X_va, _, y_va, _ = train_test_split(X_tmp, y_tmp, test_size = 0.7,         random_state = 42, stratify = y_tmp)\n",
        "    X_tr = torch.tensor(X_tr, dtype = torch.float32)\n",
        " y_tr = torch.tensor(y_tr, dtype = torch.long)\n",
        "    X_va = torch.tensor(X_va, dtype = torch.float32)\n",
        " y_va = torch.tensor(y_va, dtype = torch.long)\n",
        "    model = nn.Sequential(nn.Linear(2, 256), nn.ReLU(), nn.Dropout(p), nn.Linear(256,         2))\n",
        "    opt = torch.optim.Adam(model.parameters(), lr = 5e-3) # optimizer setup / step\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        " va = []\n",
        "    for _ in range(150):\n",
        "        model.train()\n",
        " logits = model(X_tr) # raw model scores before softmax/sigmoid\n",
        " loss = loss_fn(logits, y_tr) # training objective\n",
        "        opt.zero_grad()\n",
        " loss.backward()\n",
        " opt.step()\n",
        "        model.eval();\n",
        "        with torch.no_grad(): va.append(float(loss_fn(model(X_va), y_va)))\n",
        "    return va\n",
        "v0 = curves_dropout(0.0)\n",
        " v1 = curves_dropout(0.6)\n",
        "e = range(1, len(v0)+1)\n",
        "plt.figure(figsize = (5.8, 3.0)) # plotting # plotting  # plotting\n",
        " plt.plot(e, v0, label = 'dropout p = 0.0') # plotting # plotting  # plotting\n",
        " plt.plot(e, v1, label = 'dropout p = 0.6') # plotting # plotting  # plotting\n",
        "plt.xlabel('epoch') # plotting # plotting  # plotting\n",
        " plt.ylabel('val loss') # plotting # plotting  # plotting\n",
        " plt.legend(frameon = False) # plotting # plotting  # plotting\n",
        " plt.tight_layout() # plotting # plotting  # plotting\n",
        " plt.show() # plotting # plotting  # plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stopping (keep best validation model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_earlystop(X_tr, y_tr, X_va, y_va, *, epochs = 200, patience = 20):\n",
        "    model = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 2))\n",
        "    opt = torch.optim.Adam(model.parameters(), lr = 5e-3, \n",
        "        weight_decay = 1e-3) # optimizer setup / step\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    best_sd = None\n",
        " best = float('inf')\n",
        " wait = 0\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        " opt.zero_grad()\n",
        "        loss = loss_fn(model(X_tr), y_tr) # training objective\n",
        "        loss.backward()\n",
        " opt.step()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            va = float(loss_fn(model(X_va), y_va))\n",
        "        if va < best:\n",
        "            best = va\n",
        " wait = 0\n",
        "            # clone state dict without tying storage to model tensors\n",
        "            best_sd = {k: v.detach().cpu().clone() for k,                 v in model.state_dict().items()}\n",
        "        else:\n",
        "            wait += 1\n",
        "        if wait >= patience: break\n",
        "    model.load_state_dict(best_sd)\n",
        "    return model, best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning-rate schedules (constant, step, cosine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = np.arange(0, 100)\n",
        "lr_const = np.full_like(epochs, 5e-3, dtype = float)\n",
        "lr_step = np.where(epochs < 60, 5e-3, 2.5e-3)\n",
        "lr_cos = 0.5*(1+np.cos(np.pi*epochs/100))*5e-3\n",
        "plt.figure(figsize = (6.4, 3.0)) # plotting # plotting  # plotting\n",
        "plt.plot(epochs, lr_const, label = 'constant') # plotting # plotting  # plotting\n",
        "plt.plot(epochs, lr_step, label = 'step@60') # plotting # plotting  # plotting\n",
        "plt.plot(epochs, lr_cos, label = 'cosine') # plotting # plotting  # plotting\n",
        "plt.xlabel('epoch') # plotting # plotting  # plotting\n",
        " plt.ylabel('learning rate') # plotting # plotting  # plotting\n",
        " plt.legend(frameon = False) # plotting # plotting  # plotting\n",
        " plt.tight_layout() # plotting # plotting  # plotting\n",
        " plt.show() # plotting # plotting  # plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}