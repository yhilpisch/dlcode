{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d150d1",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7411f38",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f12c21",
   "metadata": {},
   "source": [
    "# Chapter 10 — Improving Training\n",
    "Regularization, early stopping, and LR schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib scikit-learn\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de217f",
   "metadata": {},
   "source": [
    "## Train/val curves with/without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def curves_l2(weight_decay):\n",
    "    torch.manual_seed(0)\n",
    "    X, y = make_moons(n_samples=1000, noise=0.35, random_state=0)\n",
    "    # small train split to induce overfitting and highlight L2\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.65, random_state=42, stratify=y\n",
    "    )\n",
    "    X_va, _, y_va, _ = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.65, random_state=42, stratify=y_tmp\n",
    "    )\n",
    "    X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "    X_va = torch.tensor(X_va, dtype=torch.float32)\n",
    "    y_va = torch.tensor(y_va, dtype=torch.long)\n",
    "\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(2, 256), torch.nn.ReLU(), torch.nn.Linear(256, 2))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=weight_decay)\n",
    "    tr, va = [], []\n",
    "    for _ in range(150):\n",
    "        model.train()\n",
    "        logits = model(X_tr)\n",
    "        loss = F.cross_entropy(logits, y_tr)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tr.append(float(loss))\n",
    "            va.append(float(F.cross_entropy(model(X_va), y_va)))\n",
    "            return tr, va\n",
    "\n",
    "            # Compare no L2 vs L2\n",
    "            tr0, va0 = curves_l2(0.0)\n",
    "            tr1, va1 = curves_l2(5e-3)\n",
    "\n",
    "            e = range(1, len(tr0)+1)\n",
    "            plt.figure(figsize=(7.6, 3.0))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(e, tr0, label='train'); plt.plot(e, va0, label='val')\n",
    "            plt.title('no weight decay'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(frameon=False)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(e, tr1, label='train'); plt.plot(e, va1, label='val')\n",
    "            plt.title('weight decay 0.005'); plt.xlabel('epoch'); plt.legend(frameon=False)\n",
    "            plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20ae67",
   "metadata": {},
   "source": [
    "## Dropout p=0.0 vs p=0.6 — validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01658fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def curves_dropout(p):\n",
    "    torch.manual_seed(0)\n",
    "    X, y = make_moons(n_samples=1000, noise=0.40, random_state=0)\n",
    "    # even smaller train split to surface regularization effect\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.7, random_state=42, stratify=y\n",
    "    )\n",
    "    X_va, _, y_va, _ = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.7, random_state=42, stratify=y_tmp\n",
    "    )\n",
    "    X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "    X_va = torch.tensor(X_va, dtype=torch.float32)\n",
    "    y_va = torch.tensor(y_va, dtype=torch.long)\n",
    "\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(2, 256), torch.nn.ReLU(), torch.nn.Dropout(p), torch.nn.Linear(256, 2))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "    va = []\n",
    "    for _ in range(150):\n",
    "        model.train()\n",
    "        logits = model(X_tr)\n",
    "        loss = F.cross_entropy(logits, y_tr)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va.append(float(F.cross_entropy(model(X_va), y_va)))\n",
    "            return va\n",
    "\n",
    "            v0 = curves_dropout(0.0)\n",
    "            v1 = curves_dropout(0.6)\n",
    "\n",
    "            e = range(1, len(v0)+1)\n",
    "            plt.figure(figsize=(5.8, 3.0))\n",
    "            plt.plot(e, v0, label='dropout p = 0.0')\n",
    "            plt.plot(e, v1, label='dropout p = 0.6')\n",
    "            plt.xlabel('epoch'); plt.ylabel('val loss'); plt.legend(frameon=False)\n",
    "            plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b432e",
   "metadata": {},
   "source": [
    "## Early stopping (keep best validation model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_earlystop(X_tr, y_tr, X_va, y_va, *, epochs=200, patience=20):\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(2, 32), torch.nn.ReLU(), torch.nn.Linear(32, 2))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    best_sd = None\n",
    "    best = float('inf')\n",
    "    wait = 0\n",
    "    for _ in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        loss = loss_fn(model(X_tr), y_tr); loss.backward(); opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va = float(loss_fn(model(X_va), y_va))\n",
    "            if va < best:\n",
    "                best = va\n",
    "                wait = 0\n",
    "                best_sd = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= patience:\n",
    "                        break\n",
    "                        if best_sd is not None:\n",
    "                            model.load_state_dict(best_sd)\n",
    "                            return model, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed56b7",
   "metadata": {},
   "source": [
    "## Learning-rate schedules (constant, step, cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(0, 100)\n",
    "lr_const = np.full_like(epochs, 5e-3, dtype = float)\n",
    "lr_step = np.where(epochs < 60, 5e-3, 2.5e-3)\n",
    "lr_cos = 0.5*(1+np.cos(np.pi*epochs/100))*5e-3\n",
    "plt.figure(figsize = (6.4, 3.0))  # plotting\n",
    "plt.plot(epochs, lr_const, label = 'constant')  # plotting\n",
    "plt.plot(epochs, lr_step, label = 'step@60')  # plotting\n",
    "plt.plot(epochs, lr_cos, label = 'cosine')  # plotting\n",
    "plt.xlabel('epoch')  # plotting\n",
    "plt.ylabel('learning rate')  # plotting\n",
    "plt.legend(frameon = False)  # plotting\n",
    "plt.tight_layout()  # plotting\n",
    "plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23d416",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}