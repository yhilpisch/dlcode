{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f13480c",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d00e3",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3dfb2d",
   "metadata": {},
   "source": [
    "# Chapter 12 â€” Training at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b17c8",
   "metadata": {},
   "source": [
    "## Throughput quick check (toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ae1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(256, 64, device = device)\n",
    "w = torch.randn(64, 64, device = device)\n",
    "torch.cuda.synchronize() if device.type=='cuda' else None\n",
    "t0 = time.time()\n",
    "for _ in range(500):\n",
    "    y = x @ w # targets/labels  # targets/labels\n",
    "    torch.cuda.synchronize() if device.type=='cuda' else None\n",
    "    elapsed = time.time() - t0\n",
    "    elapsed\n",
    "    # 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63398125",
   "metadata": {},
   "source": [
    "## AMP training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f90836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nn.Linear(128, 10).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)  # AMP gradient scaler\n",
    "\n",
    "x = torch.randn(32, 128, device=device)\n",
    "y = torch.randint(0, 10, (32,), device=device)  # targets/labels\n",
    "\n",
    "opt.zero_grad(set_to_none=True)\n",
    "if use_amp:\n",
    "    # Mixed precision region on CUDA\n",
    "    with torch.amp.autocast('cuda', enabled=True):  # mixed precision region\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(opt)\n",
    "    scaler.update()\n",
    "    result = 'AMP step complete on CUDA'\n",
    "else:\n",
    "    # Fallback: full precision on CPU/Metal\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    result = 'CUDA not available; AMP demo skipped.'\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c58b",
   "metadata": {},
   "source": [
    "## Gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nn.Linear(128, 10).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-2) # optimizer setup / step\n",
    "accum = 4\n",
    "opt.zero_grad(set_to_none = True)\n",
    "for step in range(8):\n",
    "    x = torch.randn(16, 128, device = device)\n",
    "    y = torch.randint(0, 10, (16, ), device = device) # targets/labels  # targets/labels\n",
    "    loss = F.cross_entropy(model(x), y) / accum # training objective\n",
    "    loss.backward()\n",
    "    if (step+1) % accum == 0:\n",
    "        opt.step()\n",
    "        opt.zero_grad(set_to_none = True)\n",
    "        # None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3b8f4",
   "metadata": {},
   "source": [
    "## Checkpoint save/load (toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d02433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "state = {'model': {'w': torch.randn(2)}, 'epoch': 3}\n",
    "torch.save(state, 'checkpoint.pt')\n",
    "ckpt = torch.load('checkpoint.pt', map_location = device)\n",
    "ckpt['epoch'], isinstance(ckpt['model'], dict)\n",
    "# (3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a0646",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n\n",
    "1. Profile a short run with AMP on/off; compare speed and memory usage.\n",
    "2. Accumulate gradients over N steps and match an equivalent batch size.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
