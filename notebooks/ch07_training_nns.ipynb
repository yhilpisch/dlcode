{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7 \u2014 Training Neural Networks\n",
        "Tiny MLP on moons: losses, training loop, optimizers, and diagnostics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install torch numpy matplotlib scikit-learn\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8') # plotting # plotting  # plotting\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal MLP and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0) # reproducibility\n",
        "X, y = make_moons(n_samples = 600, noise = 0.25, random_state = 0)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = 0.25, random_state = 42,     stratify = y)\n",
        "X_tr, X_te = torch.tensor(X_tr, dtype = torch.float32), torch.tensor(X_te,     dtype = torch.float32)\n",
        "y_tr, y_te = torch.tensor(y_tr, dtype = torch.long), torch.tensor(y_te,     dtype = torch.long)\n",
        "W1 = torch.randn(2, 16, requires_grad = True) # layer 1 weights # layer 1 weights  # layer 1 weights\n",
        " b1 = torch.zeros(16, requires_grad = True) # layer 1 bias # layer 1 bias  # layer 1 bias\n",
        "W2 = torch.randn(16, 2, requires_grad = True) # layer 2 weights # layer 2 weights  # layer 2 weights\n",
        " b2 = torch.zeros(2, requires_grad = True) # layer 2 bias # layer 2 bias  # layer 2 bias\n",
        "with torch.no_grad(): W1.mul_(0.5)\n",
        " W2.mul_(0.5)\n",
        "def forward(X):\n",
        "    h = torch.relu(X@W1 + b1) # hidden activations  # hidden activations\n",
        " return h@W2 + b2\n",
        "for _ in range(300):\n",
        "    logits = forward(X_tr) # raw model scores before softmax/sigmoid\n",
        " loss = torch.nn.functional.cross_entropy(logits, y_tr) # training objective\n",
        "    for p in (W1, b1, W2, b2):\n",
        "        if p.grad is not None: p.grad.zero_()\n",
        "    loss.backward();\n",
        "    with torch.no_grad():\n",
        "        for p in (W1, b1, W2, b2): p -= 0.1 * p.grad\n",
        "float(((forward(X_te).argmax(1)==y_te).float().mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers: SGD vs Adam (quick check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(opt_name, lr):\n",
        "    W1 = torch.randn(2, 16, requires_grad = True) # layer 1 weights # layer 1 weights  # layer 1 weights\n",
        " b1 = torch.zeros(16, requires_grad = True) # layer 1 bias # layer 1 bias  # layer 1 bias\n",
        "    W2 = torch.randn(16, 2, requires_grad = True) # layer 2 weights # layer 2 weights  # layer 2 weights\n",
        " b2 = torch.zeros(2, requires_grad = True) # layer 2 bias # layer 2 bias  # layer 2 bias\n",
        "    with torch.no_grad(): W1.mul_(0.5)\n",
        " W2.mul_(0.5)\n",
        "    opt = (torch.optim.SGD([W1, b1, W2, b2], # optimizer setup / step\n",
        "        lr = lr) if opt_name=='sgd' else torch.optim.Adam([W1, b1, W2, b2], lr = lr))\n",
        "    for _ in range(200):\n",
        "        logits = torch.relu(X_tr@W1 + b1)@W2 + b2 # raw model scores before softmax/sigmoid\n",
        "        loss = torch.nn.functional.cross_entropy(logits, y_tr) # training objective\n",
        "        opt.zero_grad()\n",
        " loss.backward()\n",
        " opt.step()\n",
        "    return float((((torch.relu(X_te@W1 + b1)@W2 + b2).argmax(1)==y_te).float().mean()))\n",
        "run('sgd', 0.1), run('adam', 0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}