{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58db89e",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dd7bf",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9368336",
   "metadata": {},
   "source": [
    "# Chapter 7 â€” Training Neural Networks\n",
    "Tiny MLP on moons: losses, training loop, optimizers, and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib scikit-learn\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463eb2d",
   "metadata": {},
   "source": [
    "## Minimal MLP and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "torch.manual_seed(0)\n",
    "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
    "X_te = torch.tensor(X_te, dtype=torch.float32)\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "y_te = torch.tensor(y_te, dtype=torch.long)\n",
    "\n",
    "# Two-layer MLP params (leaf tensors with grads)\n",
    "W1 = torch.randn(2, 16, requires_grad=True)\n",
    "b1 = torch.zeros(16, requires_grad=True)\n",
    "W2 = torch.randn(16, 2, requires_grad=True)\n",
    "b2 = torch.zeros(2, requires_grad=True)\n",
    "\n",
    "# Light init scaling without tracking gradients\n",
    "with torch.no_grad():\n",
    "    W1.mul_(0.5)\n",
    "    W2.mul_(0.5)\n",
    "\n",
    "    # Forward function\n",
    "\n",
    "    def forward(X):\n",
    "        h = torch.relu(X @ W1 + b1)\n",
    "        return h @ W2 + b2\n",
    "\n",
    "        # Manual SGD loop\n",
    "        for _ in range(300):\n",
    "            logits = forward(X_tr)\n",
    "            loss = F.cross_entropy(logits, y_tr)\n",
    "            for p in (W1, b1, W2, b2):\n",
    "                if p.grad is not None:\n",
    "                    p.grad.zero_()\n",
    "                    loss.backward()\n",
    "                    with torch.no_grad():\n",
    "                        for p in (W1, b1, W2, b2):\n",
    "                            p -= 0.1 * p.grad\n",
    "\n",
    "                            # Evaluate accuracy on test set\n",
    "                            acc = float((forward(X_te).argmax(1) == y_te).float().mean())\n",
    "                            acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70025e29",
   "metadata": {},
   "source": [
    "## Optimizers: SGD vs Adam (quick check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run(opt_name, lr):\n",
    "    # Fresh params each run\n",
    "    W1 = torch.randn(2, 16, requires_grad=True)\n",
    "    b1 = torch.zeros(16, requires_grad=True)\n",
    "    W2 = torch.randn(16, 2, requires_grad=True)\n",
    "    b2 = torch.zeros(2, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        W1.mul_(0.5)\n",
    "        W2.mul_(0.5)\n",
    "        params = [W1, b1, W2, b2]\n",
    "        opt = torch.optim.SGD(params, lr=lr) if opt_name == 'sgd' else torch.optim.Adam(params, lr=lr)\n",
    "        for _ in range(200):\n",
    "            logits = torch.relu(X_tr @ W1 + b1) @ W2 + b2\n",
    "            loss = F.cross_entropy(logits, y_tr)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            acc = float(((torch.relu(X_te @ W1 + b1) @ W2 + b2).argmax(1) == y_te).float().mean())\n",
    "            return acc\n",
    "\n",
    "            sgd_acc = run('sgd', 0.1)\n",
    "            adam_acc = run('adam', 0.01)\n",
    "            sgd_acc, adam_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9a3cb",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}