{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24e260f3",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8dd7bf",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n",
        "\n",
        "**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9368336",
      "metadata": {},
      "source": [
        "# Chapter 7 â€” Training Neural Networks\n",
        "Tiny MLP on moons: losses, training loop, optimizers, and diagnostics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f038ec",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
        "Use it as a companion to the chapter: run each cell, read the short notes,\n",
        "and try small variations to build intuition.\n",
        "\n",
        "Tips:\n",
        "- Run cells top to bottom; restart kernel if state gets confusing.\n",
        "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
        "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625cb319",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install torch numpy matplotlib scikit-learn\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')  # plotting\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9463eb2d",
      "metadata": {},
      "source": [
        "## Minimal MLP and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5494ac13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch  # core tensor/autograd library\n",
        "import torch.nn.functional as F  # functional ops: loss and activation\n",
        "from sklearn.datasets import make_moons  # toy dataset generator\n",
        "from sklearn.model_selection import train_test_split  # train/test split\n",
        "\n",
        "# Data\n",
        "torch.manual_seed(0)  # fix RNG for reproducibility\n",
        "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)  # features, labels\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(  # stratified split\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "X_tr = torch.tensor(X_tr, dtype=torch.float32)  # train features tensor\n",
        "X_te = torch.tensor(X_te, dtype=torch.float32)  # test features tensor\n",
        "y_tr = torch.tensor(y_tr, dtype=torch.long)     # train labels (class indices)\n",
        "y_te = torch.tensor(y_te, dtype=torch.long)     # test labels (class indices)\n",
        "\n",
        "# Two-layer MLP params (leaf tensors with grads)\n",
        "W1 = torch.randn(2, 16, requires_grad=True)  # layer1 weights (2->16)\n",
        "b1 = torch.zeros(16, requires_grad=True)      # layer1 bias (16,)\n",
        "W2 = torch.randn(16, 2, requires_grad=True)  # layer2 weights (16->2)\n",
        "b2 = torch.zeros(2, requires_grad=True)      # layer2 bias (2,)\n",
        "\n",
        "# Light init scaling without tracking gradients\n",
        "with torch.no_grad():\n",
        "    W1.mul_(0.5)  # halve initial scale\n",
        "    W2.mul_(0.5)  # halve initial scale\n",
        "\n",
        "def forward(X):  # manual forward pass\n",
        "    h = torch.relu(X @ W1 + b1)  # hidden activations after ReLU\n",
        "    return h @ W2 + b2           # logits for 2 classes\n",
        "\n",
        "# Manual SGD loop\n",
        "for _ in range(300):  # epochs\n",
        "    logits = forward(X_tr)                # compute logits on train set\n",
        "    loss = F.cross_entropy(logits, y_tr)  # CE loss on train set\n",
        "    for p in (W1, b1, W2, b2):            # iterate parameters\n",
        "        if p.grad is not None:            # clear stale grads\n",
        "            p.grad.zero_()\n",
        "    loss.backward()                       # backprop gradients\n",
        "    with torch.no_grad():                 # parameter update (no grad)\n",
        "        for p in (W1, b1, W2, b2):\n",
        "            p -= 0.1 * p.grad            # SGD step (lr=0.1)\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "with torch.no_grad():\n",
        "    acc = float((forward(X_te).argmax(1) == y_te).float().mean())  # test accuracy\n",
        "acc  # display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70025e29",
      "metadata": {},
      "source": [
        "## Optimizers: SGD vs Adam (quick check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e439b610",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch  # tensor library and autograd\n",
        "import torch.nn.functional as F  # functional API for loss, activations\n",
        "\n",
        "def run(opt_name, lr):  # train tiny MLP params with chosen optimizer\n",
        "    # Fresh parameters (two-layer MLP: 2->16->2)\n",
        "    W1 = torch.randn(2, 16, requires_grad=True)  # first layer weights\n",
        "    b1 = torch.zeros(16, requires_grad=True)     # first layer bias\n",
        "    W2 = torch.randn(16, 2, requires_grad=True)  # second layer weights\n",
        "    b2 = torch.zeros(2, requires_grad=True)      # second layer bias\n",
        "    # Light scaling for stable starts (no grad tracking here)\n",
        "    with torch.no_grad():\n",
        "        W1.mul_(0.5)  # halve initial scale\n",
        "        W2.mul_(0.5)  # halve initial scale\n",
        "    params = [W1, b1, W2, b2]  # parameter list for optimizer\n",
        "    # Create optimizer from name\n",
        "    optimizer = (\n",
        "        torch.optim.SGD(params, lr=lr)    # SGD variant\n",
        "        if opt_name == 'sgd'             # select by string\n",
        "        else torch.optim.Adam(params, lr=lr)  # Adam variant\n",
        "    )\n",
        "    for _ in range(200):  # training epochs\n",
        "        # Forward pass: ReLU hidden then linear to logits\n",
        "        logits = torch.relu(X_tr @ W1 + b1) @ W2 + b2\n",
        "        loss = F.cross_entropy(logits, y_tr)  # CE loss on training batch\n",
        "        optimizer.zero_grad()  # clear accumulated gradients\n",
        "        loss.backward()        # backprop gradients into params\n",
        "        optimizer.step()       # update parameters\n",
        "    # Evaluate on test split (no grad tracking)\n",
        "    with torch.no_grad():\n",
        "        logits_te = torch.relu(X_te @ W1 + b1) @ W2 + b2  # test logits\n",
        "        acc = float((logits_te.argmax(1) == y_te).float().mean())  # test accuracy\n",
        "    return acc  # scalar accuracy\n",
        "\n",
        "# Compare SGD vs Adam on test accuracy\n",
        "sgd_acc = run('sgd', 0.1)   # SGD at lr=0.1\n",
        "adam_acc = run('adam', 0.01)  # Adam at lr=1e-2\n",
        "sgd_acc, adam_acc  # show both accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and validation curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F  # PyTorch core APIs\n",
        "import matplotlib.pyplot as plt  # plotting backend\n",
        "# Define a fresh tiny MLP model for logging\n",
        "model2 = nn.Sequential(  # 2->16->2 classifier\n",
        "    nn.Linear(2, 16),  # first affine layer\n",
        "    nn.ReLU(),         # nonlinearity\n",
        "    nn.Linear(16, 2)   # logits for 2 classes\n",
        ")\n",
        "opt2 = torch.optim.Adam(model2.parameters(), lr=3e-3)  # optimizer\n",
        "tr_hist, va_hist = [], []  # containers for train/validation losses\n",
        "for _ in range(60):  # training epochs\n",
        "    model2.train(); opt2.zero_grad()  # enable train mode; clear grads\n",
        "    tr_logits = model2(X_tr); tr_loss = F.cross_entropy(tr_logits, y_tr)  # loss on train\n",
        "    tr_loss.backward(); opt2.step()  # backprop and parameter update\n",
        "    model2.eval()  # switch to eval mode for validation\n",
        "    with torch.no_grad():  # no grads for validation\n",
        "        va_loss = F.cross_entropy(model2(X_te), y_te).item()  # numeric val loss\n",
        "    tr_hist.append(float(tr_loss.detach())); va_hist.append(va_loss)  # log losses\n",
        "plt.figure(figsize=(5.5, 2.8))  # compact figure\n",
        "plt.plot(tr_hist, label='train loss'); plt.plot(va_hist, '--', label='val loss')  # curves\n",
        "plt.xlabel('epoch'); plt.ylabel('loss')  # axis labels\n",
        "plt.legend(frameon=False); plt.tight_layout(); plt.show()  # legend and render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision boundary (after training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, matplotlib.pyplot as plt  # NumPy + plotting\n",
        "# Concatenate splits to get plotting bounds\n",
        "allX = torch.cat([X_tr, X_te], dim=0).numpy()  # (N, 2) array\n",
        "# Build grid for dense evaluation\n",
        "xx, yy = np.meshgrid(  # coordinate grids\n",
        "    np.linspace(allX[:,0].min()-1, allX[:,0].max()+1, 300),  # x range\n",
        "    np.linspace(allX[:,1].min()-1, allX[:,1].max()+1, 300)   # y range\n",
        ")\n",
        "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)  # (M,2) points\n",
        "model2.eval()  # eval mode for deterministic behavior\n",
        "with torch.no_grad():  # inference-only\n",
        "    zz = model2(grid).argmax(1).reshape(xx.shape).numpy()  # predicted class per cell\n",
        "plt.figure(figsize=(5.2, 3.6))  # figure size\n",
        "plt.contourf(xx, yy, zz, alpha=0.25, cmap='coolwarm')  # decision regions\n",
        "plt.scatter(X_te[:,0], X_te[:,1], c=y_te, s=12, edgecolor='k')  # test points\n",
        "plt.title('Decision boundary (TinyMLP)')  # title\n",
        "plt.tight_layout(); plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e55ea163",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. Train for a fixed budget and compare losses across batch sizes.\n",
        "2. Change optimizer or learning rate schedule and compare final metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4156720b",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}