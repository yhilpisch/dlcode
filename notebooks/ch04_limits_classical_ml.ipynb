{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa50d68",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99562e23",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0cb4a",
   "metadata": {},
   "source": [
    "# Chapter 4 â€” The Limits of Classical ML\n",
    "This Colab-ready notebook mirrors the figures and small experiments from the chapter: distance concentration, polynomial feature growth, kernel scaling, complexity vs depth, learning curves, and residual patterns when the model is mis-specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41daa30",
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Optional: Colab usually has these\n",
    "# !pip -q install numpy matplotlib scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0890a3e",
   "metadata": {},
   "source": [
    "## Distance concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77744acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "dims = np.array([2, 5, 10, 20, 50, 100])\n",
    "n_points = 500\n",
    "rel = []\n",
    "\n",
    "for d in dims:\n",
    "    # Sample n_points points uniformly in [0,1]^d\n",
    "    X = rng.random((n_points, int(d)))\n",
    "    # Subsample indices to estimate min/max distances\n",
    "    idx = rng.choice(n_points, size=60, replace=False)\n",
    "    mins, maxs = [], []\n",
    "    for i in idx:\n",
    "        diffs = X - X[i]\n",
    "        dists = np.sqrt(np.sum(diffs * diffs, axis=1))\n",
    "        dists = dists[dists > 0]\n",
    "        mins.append(dists.min())\n",
    "        maxs.append(dists.max())\n",
    "        # Relative contrast for this dimension d\n",
    "        rel.append((np.mean(maxs) - np.mean(mins)) / np.mean(mins))\n",
    "\n",
    "        # Plot once after accumulating all contrasts\n",
    "        plt.figure(figsize=(5, 3.2))\n",
    "        plt.plot(dims, rel, marker='o')\n",
    "        plt.xlabel('dimension d')\n",
    "        plt.ylabel('relative contrast')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d49d03",
   "metadata": {},
   "source": [
    "## Polynomial feature growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def poly_count(d, K):\n",
    "    return sum(math.comb(d+k-1, k) for k in range(1, K+1))\n",
    "    d = 20\n",
    "    Ks = np.arange(1, 8)\n",
    "    counts = [poly_count(d, int(K)) for K in Ks]\n",
    "    plt.figure(figsize = (5, 3.2))  # plotting\n",
    "    plt.plot(Ks, counts, marker = 'o')  # plotting\n",
    "    plt.yscale('log')  # plotting\n",
    "    plt.xlabel('degree K')  # plotting\n",
    "    plt.ylabel('  # features (log)')\n",
    "    plt.tight_layout()  # plotting\n",
    "    plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fc38d",
   "metadata": {},
   "source": [
    "## Kernel scaling (n^2 memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6273470",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.array([200, 500, 1000, 2000, 5000, 10000])\n",
    "mem_gb = (n.astype(float)**2 * 8) / (1024**3)\n",
    "plt.figure(figsize = (5, 3.2))  # plotting\n",
    "plt.plot(n, mem_gb, marker = 'o')  # plotting\n",
    "plt.xlabel('samples n')  # plotting\n",
    "plt.ylabel('Kernel matrix memory (GB)')  # plotting\n",
    "plt.tight_layout()  # plotting\n",
    "plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38ad9e",
   "metadata": {},
   "source": [
    "## Complexity vs depth (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "depths = range(1, 16)\n",
    "tr, te = [], []\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=0).fit(X_tr, y_tr)\n",
    "    tr.append(clf.score(X_tr, y_tr))\n",
    "    te.append(clf.score(X_te, y_te))\n",
    "\n",
    "    plt.figure(figsize=(5.2, 3.2))\n",
    "    plt.plot(depths, tr, marker='o', label='train')\n",
    "    plt.plot(depths, te, marker='o', label='test')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fa967",
   "metadata": {},
   "source": [
    "## Learning curve (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2237677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "X, y = make_moons(n_samples = 2000, noise = 0.25, random_state = 0)\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter = 2000))\n",
    "sizes, tr, te = learning_curve(model, X, y, cv = 5, train_sizes = np.linspace(0.1, 1.0,     8))\n",
    "plt.figure(figsize = (5.2, 3.2))  # plotting\n",
    "plt.plot(sizes, tr.mean(axis = 1), marker = 'o', label = 'train')  # plotting\n",
    "plt.plot(sizes, te.mean(axis = 1), marker = 'o', label = 'test')  # plotting\n",
    "plt.legend(frameon = False)  # plotting\n",
    "plt.xlabel('training size')  # plotting\n",
    "plt.ylabel('accuracy')  # plotting\n",
    "plt.tight_layout()  # plotting\n",
    "plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b12864",
   "metadata": {},
   "source": [
    "## Residuals under mis-specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)  # RNG setup\n",
    "x = np.linspace(-3, 3, 80)\n",
    "y_true = 0.6*x**2 - 0.5*x + 0.3\n",
    "y = y_true + rng.normal(0, 0.5, size = x.shape)  # targets/labels\n",
    "A = np.vstack([x, np.ones_like(x)]).T\n",
    "a, b = np.linalg.lstsq(A, y, rcond = None)[0]\n",
    "resid = y - (a*x + b)\n",
    "plt.figure(figsize = (5.2, 3.2))  # plotting\n",
    "plt.scatter(x, resid, s = 18)  # plotting\n",
    "plt.axhline(0, color = 'k', lw = 1)  # plotting\n",
    "plt.xlabel('x')  # plotting\n",
    "plt.ylabel('residual')  # plotting\n",
    "plt.tight_layout()  # plotting\n",
    "plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e91b5",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}