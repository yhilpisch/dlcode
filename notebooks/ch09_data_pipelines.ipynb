{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 9 \u2014 Working with Data in PyTorch\n",
        "Datasets, DataLoaders, transforms, and batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install torch numpy matplotlib scikit-learn\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and DataLoader (moons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0) # reproducibility\n",
        "X, y = make_moons(n_samples = 600, noise = 0.25, random_state = 0)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = 0.25, random_state = 42,     stratify = y)\n",
        "X_tr, X_te = torch.tensor(X_tr, dtype = torch.float32), torch.tensor(X_te,     dtype = torch.float32)\n",
        "y_tr, y_te = torch.tensor(y_tr, dtype = torch.long), torch.tensor(y_te,     dtype = torch.long)\n",
        "train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size = 64,   # wrap tensors as a dataset\n",
        "    shuffle = True) # wrap tensors as a dataset\n",
        "test_loader = DataLoader(TensorDataset(X_te, y_te),   # wrap tensors as a dataset\n",
        "    batch_size = 256) # wrap tensors as a dataset\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self): super().__init__()\n",
        " self.net = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 2))\n",
        "    def forward(self, x): return self.net(x)\n",
        "model = TinyMLP()\n",
        "opt = torch.optim.Adam(model.parameters(), lr = 5e-3) # optimizer setup / step\n",
        " loss_fn = nn.CrossEntropyLoss()\n",
        "for _ in range(10):\n",
        "    model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        loss = loss_fn(model(Xb), yb) # training objective\n",
        " opt.zero_grad()\n",
        " loss.backward()\n",
        " opt.step()\n",
        "model.eval()\n",
        " float(((model(X_te).argmax(1)==y_te).float().mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform: standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Standardize:\n",
        "    def __init__(self, mean, std): self.mean = mean\n",
        " self.std = std\n",
        "    def __call__(self, x): return (x-self.mean)/(self.std+1e-8)\n",
        "mu, sigma = X_tr.mean(0), X_tr.std(0)\n",
        "std = Standardize(mu, sigma)\n",
        "X_tr_s, X_te_s = std(X_tr), std(X_te)\n",
        "train_loader = DataLoader(TensorDataset(X_tr_s, y_tr), batch_size = 64,   # wrap tensors as a dataset\n",
        "    shuffle = True) # wrap tensors as a dataset\n",
        "test_loader = DataLoader(TensorDataset(X_te_s, y_te),   # wrap tensors as a dataset\n",
        "    batch_size = 256) # wrap tensors as a dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom collate (variable length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToySeq(Dataset):\n",
        "    def __init__(self, rng, n = 20):\n",
        "        self.x = [torch.tensor(rng.integers(1, 10, size = rng.integers(3,             8))) for _ in range(n)]\n",
        "        self.y = [xi.sum()%2 for xi in self.x]\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i].float(), int(self.y[i])\n",
        "def pad_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        " L = max(x.size(0) for x in xs)\n",
        " Xp = torch.zeros(len(xs), L)\n",
        "    for i, x in enumerate(xs): Xp[i, :x.size(0)] = x\n",
        "    return Xp, torch.tensor(ys, dtype = torch.long)\n",
        "rng = np.random.default_rng(0) # RNG setup\n",
        " seq_loader = DataLoader(ToySeq(rng), batch_size = 4,   # create data loader\n",
        "     collate_fn = pad_collate) # create data loader\n",
        "xb, yb = next(iter(seq_loader))\n",
        " xb.shape, yb.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}