{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4673309b",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c0087",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1240c3",
   "metadata": {},
   "source": [
    "# Chapter 9 â€” Working with Data in PyTorch\n",
    "Datasets, DataLoaders, transforms, and batching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780a898",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n",
    "\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required packages are available (already installed in this env)\n",
    "# !pip -q install torch numpy matplotlib scikit-learn\n",
    "import torch, numpy as np, matplotlib.pyplot as plt  # PyTorch, NumPy, and plotting\n",
    "from torch import nn  # Neural network modules\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset  # Dataset and DataLoader utilities\n",
    "from sklearn.datasets import make_moons  # Synthetic 2D classification dataset\n",
    "from sklearn.model_selection import train_test_split  # Train/test split helper\n",
    "# Render Matplotlib plots with high-DPI for clarity in the notebook\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587d362",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader (moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility for PyTorch ops\n",
    "torch.manual_seed(0)  # set global RNG seed\n",
    "\n",
    "# Create a two-moons toy dataset for binary classification\n",
    "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)  # features (N,2) and labels (N,)\n",
    "\n",
    "# Stratified split to preserve class balance in train and test\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(  # split into train/test subsets\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y  # 75% train / 25% test with fixed seed\n",
    ")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors with appropriate dtypes\n",
    "X_tr = torch.tensor(X_tr, dtype=torch.float32)  # training features as float32\n",
    "X_te = torch.tensor(X_te, dtype=torch.float32)  # test features as float32\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.long)  # training labels as class indices\n",
    "y_te = torch.tensor(y_te, dtype=torch.long)  # test labels as class indices\n",
    "\n",
    "# Wrap tensors in TensorDataset + DataLoader for mini-batch iteration\n",
    "train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=64, shuffle=True)  # shuffled mini-batches\n",
    "test_loader  = DataLoader(TensorDataset(X_te, y_te), batch_size=256)  # larger eval batches\n",
    "\n",
    "# Define a tiny MLP classifier: 2 -> 16 -> 2 with ReLU\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(  # sequential container of layers\n",
    "            nn.Linear(2, 16),  # input layer from 2 features to 16 hidden units\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.Linear(16, 2)  # output logits for 2 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # forward pass delegates to sequential network\n",
    "\n",
    "# Instantiate model, optimizer, and loss function\n",
    "model = TinyMLP()  # MLP instance\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-3)  # Adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # multi-class cross entropy on logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0699c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a small number of epochs over mini-batches\n",
    "for epoch in range(10):  # iterate over epochs\n",
    "    model.train()  # set model to training mode\n",
    "    for Xb, yb in train_loader:  # iterate over mini-batches\n",
    "        logits = model(Xb)  # forward pass to get class logits\n",
    "        loss = loss_fn(logits, yb)  # compute cross-entropy loss\n",
    "        opt.zero_grad()  # clear previous gradients\n",
    "        loss.backward()  # backpropagate to compute gradients\n",
    "        opt.step()  # update parameters\n",
    "\n",
    "# Evaluate simple accuracy on the test set\n",
    "model.eval()  # set model to eval mode\n",
    "with torch.no_grad():  # disable autograd for evaluation\n",
    "    accuracy = (model(X_te).argmax(1) == y_te).float().mean().item()  # compute accuracy\n",
    "accuracy  # display result in notebook output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ae2b5",
   "metadata": {},
   "source": [
    "## Transform: standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple standardization callable usable as a transform\n",
    "class Standardize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean  # per-feature mean (tensor or array)\n",
    "        self.std = std  # per-feature std (tensor or array)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-8)  # normalize and avoid division by zero\n",
    "\n",
    "# Fit transform parameters on training data and apply to train/test\n",
    "mu, sigma = X_tr.mean(0), X_tr.std(0)  # per-feature mean and std on train set\n",
    "std = Standardize(mu, sigma)  # create transform instance\n",
    "X_tr_s, X_te_s = std(X_tr), std(X_te)  # standardized tensors\n",
    "\n",
    "# Rebuild loaders with standardized features (labels unchanged)\n",
    "train_loader = DataLoader(  # training loader with standardized inputs\n",
    "    TensorDataset(X_tr_s, y_tr), batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(  # test loader with standardized inputs\n",
    "    TensorDataset(X_te_s, y_te), batch_size=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6504e5b",
   "metadata": {},
   "source": [
    "## Custom collate (variable length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy sequence dataset of variable-length sequences and parity labels\n",
    "class ToySeq(Dataset):\n",
    "    def __init__(self, rng, n=20):\n",
    "        # Generate n sequences of random integers in [1, 9], length between 3 and 7\n",
    "        self.x = [torch.tensor(rng.integers(1, 10, size=rng.integers(3, 8))) for _ in range(n)]\n",
    "        # Label is parity (even/odd) of the sum across each sequence\n",
    "        self.y = [int(xi.sum() % 2) for xi in self.x]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)  # number of sequences\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i].float(), self.y[i]  # return float tensor and integer label\n",
    "\n",
    "# Collate function to left-pad sequences in a batch to the same length\n",
    "\n",
    "def pad_collate(batch):\n",
    "    xs, ys = zip(*batch)  # unzip sequences and labels\n",
    "    L = max(x.size(0) for x in xs)  # max length in batch\n",
    "    Xp = torch.zeros(len(xs), L)  # padded batch tensor (B, L)\n",
    "    for i, x in enumerate(xs):  # copy each sequence to the left-aligned slice\n",
    "        Xp[i, :x.size(0)] = x  # pad with zeros to the right\n",
    "    return Xp, torch.tensor(ys, dtype=torch.long)  # stacked padded tensor and labels\n",
    "\n",
    "# Example usage: build a loader and fetch a sample batch\n",
    "rng = np.random.default_rng(0)  # deterministic NumPy RNG\n",
    "seq_loader = DataLoader(ToySeq(rng), batch_size=4, collate_fn=pad_collate)  # DataLoader with custom collate\n",
    "xb, yb = next(iter(seq_loader))  # take first batch\n",
    "xb.shape, yb.shape  # show padded shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7c7d3",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "- Two-moons train/test scatter\n",
    "- TinyMLP decision boundary\n",
    "- Standardization: histograms and scatter\n",
    "- ToySeq padded batch heatmap + lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the two-moons dataset: train vs test split\n",
    "import numpy as np  # ensure NumPy is available as np\n",
    "plt.figure(figsize=(5, 4))  # create a small figure\n",
    "plt.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap='coolwarm', s=12, alpha=0.8, label='train')  # train points\n",
    "plt.scatter(X_te[:, 0], X_te[:, 1], c=y_te, cmap='coolwarm', s=20, alpha=0.8, marker='x', label='test')  # test points\n",
    "plt.title('Two Moons: Train/Test Split')  # figure title\n",
    "plt.xlabel('x1')  # x-axis label\n",
    "plt.ylabel('x2')  # y-axis label\n",
    "plt.legend(loc='best', frameon=False)  # add legend without frame\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()  # render the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary of the trained TinyMLP on the original (unstandardized) features\n",
    "model.eval()  # evaluation mode for deterministic layers\n",
    "with torch.no_grad():  # no gradient tracking during visualization\n",
    "    # Build a grid spanning the feature space with some padding\n",
    "    x_min, x_max = float(X_tr[:, 0].min()) - 0.5, float(X_tr[:, 0].max()) + 0.5  # x-range\n",
    "    y_min, y_max = float(X_tr[:, 1].min()) - 0.5, float(X_tr[:, 1].max()) + 0.5  # y-range\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))  # grid\n",
    "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)  # grid as tensor\n",
    "    zz = model(grid).argmax(1).view(xx.shape).cpu().numpy()  # class predictions over grid\n",
    "\n",
    "plt.figure(figsize=(5, 4))  # create a figure\n",
    "plt.contourf(xx, yy, zz, levels=1, cmap='coolwarm', alpha=0.3)  # filled regions by predicted class\n",
    "plt.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap='coolwarm', s=10, alpha=0.9, edgecolors='none')  # training points\n",
    "plt.title('Decision Boundary (TinyMLP)')  # title\n",
    "plt.xlabel('x1')  # x-axis label\n",
    "plt.ylabel('x2')  # y-axis label\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()  # render the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19dd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions before and after standardization\n",
    "# Convert tensors to NumPy for histogram plotting\n",
    "Xtr_np = X_tr.numpy()  # original training features as NumPy\n",
    "XtrS_np = X_tr_s.numpy()  # standardized training features as NumPy\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))  # two subplots for two features\n",
    "for i, ax in enumerate(axes):  # iterate over features 0 and 1\n",
    "    ax.hist(Xtr_np[:, i], bins=30, density=True, alpha=0.6, label='raw')  # raw hist\n",
    "    ax.hist(XtrS_np[:, i], bins=30, density=True, alpha=0.6, label='standardized')  # standardized hist\n",
    "    ax.set_title(f'Feature {i} distribution')  # subplot title\n",
    "    ax.set_xlabel(f'x{i+1}')  # x-axis label\n",
    "    ax.set_ylabel('density')  # y-axis label\n",
    "    ax.legend(frameon=False)  # legend without frame\n",
    "fig.suptitle('Standardization Effect on Features', y=1.05)  # figure title\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()  # render the plot\n",
    "\n",
    "# Side-by-side scatter to show scaling and shape preservation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))  # two side-by-side plots\n",
    "ax1.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap='coolwarm', s=10, alpha=0.8)  # raw scatter\n",
    "ax1.set_title('Raw features')  # title for raw\n",
    "ax1.set_xlabel('x1')  # x label\n",
    "ax1.set_ylabel('x2')  # y label\n",
    "ax2.scatter(X_tr_s[:, 0], X_tr_s[:, 1], c=y_tr, cmap='coolwarm', s=10, alpha=0.8)  # standardized scatter\n",
    "ax2.set_title('Standardized features')  # title for standardized\n",
    "ax2.set_xlabel('x1 (standardized)')  # x label\n",
    "ax2.set_ylabel('x2 (standardized)')  # y label\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()  # render\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a122e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a padded ToySeq batch: values heatmap and per-sequence lengths\n",
    "# Compute lengths by counting non-zero entries (since we used zero-padding)\n",
    "lengths = (xb > 0).sum(dim=1).cpu().numpy()  # length of each sequence in the batch\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3), gridspec_kw={'width_ratios': [2, 1]})  # two-panel figure\n",
    "im = ax1.imshow(xb.cpu().numpy(), aspect='auto', cmap='viridis')  # show padded batch as image\n",
    "ax1.set_title('Padded batch (values)')  # title for heatmap\n",
    "ax1.set_xlabel('time step')  # x-axis label\n",
    "ax1.set_ylabel('sequence (batch index)')  # y-axis label\n",
    "plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)  # color bar for value scale\n",
    "\n",
    "ax2.bar(np.arange(len(lengths)), lengths, color='tab:blue')  # bar chart of lengths\n",
    "ax2.set_title('Sequence lengths')  # title for bar chart\n",
    "ax2.set_xlabel('batch index')  # x-axis label\n",
    "ax2.set_ylabel('length')  # y-axis label\n",
    "ax2.set_ylim(0, xb.size(1))  # make y-limit consistent with max padded length\n",
    "plt.tight_layout()  # adjust layout\n",
    "plt.show()  # render figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae583a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Write a tiny custom Dataset; use a DataLoader and inspect batching/padding.\n",
    "2. Measure throughput for two DataLoader settings (num_workers, pin_memory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc9679",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
