{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n\n**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 â€” Building Blocks of Neural Networks\n",
    "Colab-ready notebook covering neurons, activations, manual forward passes, and XOR decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib scikit-learn\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8') # plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-4, 4, steps = 9)\n",
    "torch.sigmoid(x), torch.tanh(x), torch.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "plt.figure(figsize = (5, 3)) # plotting\n",
    "plt.plot(x, 1/(1+np.exp(-x)), label = 'sigmoid') # plotting\n",
    "plt.plot(x, np.tanh(x), label = 'tanh') # plotting\n",
    "plt.plot(x, np.maximum(0, x), label = 'ReLU') # plotting\n",
    "plt.legend(frameon = False) # plotting\n",
    "plt.tight_layout() # plotting\n",
    "plt.show() # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual forward for a 2-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # reproducibility\n",
    "x = torch.randn(5, 2)\n",
    "W1 = torch.randn(2, 4) # layer 1 weights # layer 1 weights  # layer 1 weights\n",
    "b1 = torch.randn(4) # layer 1 bias # layer 1 bias  # layer 1 bias\n",
    "W2 = torch.randn(4, 1) # layer 2 weights # layer 2 weights  # layer 2 weights\n",
    "b2 = torch.randn(1) # layer 2 bias # layer 2 bias  # layer 2 bias\n",
    "h = torch.relu(x @ W1 + b1) # hidden activations  # hidden activations\n",
    "y = h @ W2 + b2 # targets/labels # targets/labels  # targets/labels\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR decision boundary: linear vs 2-layer MLP (quick demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0) # RNG setup\n",
    "X = rng.uniform(-1, 1, size = (600, 2)) # inputs # inputs  # inputs\n",
    "y = ((X[:, 0]>0) ^ (X[:, 1]>0)).astype(int) # targets/labels # targets/labels  # targets/labels\n",
    "X = X + rng.normal(0, 0.15, size = X.shape) # inputs # inputs  # inputs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lin = LogisticRegression().fit(X, y) # fit a linear classifier\n",
    "W1 = torch.randn(2, 8, requires_grad = True) # layer 1 weights # layer 1 weights  # layer 1 weights\n",
    "b1 = torch.zeros(8, requires_grad = True) # layer 1 bias # layer 1 bias  # layer 1 bias\n",
    "W2 = torch.randn(8, 2, requires_grad = True) # layer 2 weights # layer 2 weights  # layer 2 weights\n",
    "b2 = torch.zeros(2, requires_grad = True) # layer 2 bias # layer 2 bias  # layer 2 bias\n",
    "with torch.no_grad():\n",
    "    W1.mul_(0.5)\n",
    "    W2.mul_(0.5)\n",
    "    X_t = torch.tensor(X, dtype = torch.float32)\n",
    "    y_t = torch.tensor(y, dtype = torch.long)\n",
    "    for _ in range(1500):\n",
    "        h = torch.relu(X_t@W1 + b1) # hidden activations  # hidden activations\n",
    "        logits = h@W2 + b2 # raw model scores before softmax/sigmoid\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y_t) # training objective\n",
    "        for p in (W1, b1, W2, b2):\n",
    "            if p.grad is not None: p.grad.zero_()\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in (W1, b1, W2, b2): p -= 0.1*p.grad\n",
    "                xmin, xmax = X[:, 0].min()-0.4, X[:, 0].max()+0.4\n",
    "                ymin, ymax = X[:, 1].min()-0.4, X[:, 1].max()+0.4\n",
    "                xx, yy = np.meshgrid(np.linspace(xmin, xmax, 200), np.linspace(ymin, ymax, 200))\n",
    "                grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "                with torch.no_grad():\n",
    "                    h = torch.relu(torch.tensor(grid,   # hidden activations\n",
    "                    dtype = torch.float32)@W1 + b1) # hidden activations\n",
    "                    zz_mlp = (h@W2 + b2).argmax(dim = 1).numpy().reshape(xx.shape)\n",
    "                    zz_lin = lin.predict(grid).reshape(xx.shape)\n",
    "                    fig, ax = plt.subplots(1, 2, figsize = (8, 3), sharex = True,   # plotting\n",
    "                    sharey = True) # plotting\n",
    "                    for a, zz, title in zip(ax, [zz_lin, zz_mlp], ['Linear', '2-layer MLP']):\n",
    "                        a.contourf(xx, yy, zz, levels = [-0.5, 0.5, 1.5], cmap = 'coolwarm', alpha = 0.25)\n",
    "                        a.scatter(X[y==0, 0], X[y==0, 1], s = 8)\n",
    "                        a.scatter(X[y==1, 0], X[y==1, 1], s = 8)\n",
    "                        a.set_title(title)\n",
    "                        plt.tight_layout() # plotting\n",
    "                        plt.show() # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}