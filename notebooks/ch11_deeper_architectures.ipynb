{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8932bc0b",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa3021",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644186e6",
   "metadata": {},
   "source": [
    "# Chapter 11 â€” Deeper Architectures\n",
    "Convolutions, pooling, and a tiny CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5740f56",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n",
    "\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib  # install dependencies if needed\n",
    "import matplotlib.pyplot as plt  # plotting utilities\n",
    "import numpy as np  # numerical helpers\n",
    "import torch  # tensor library\n",
    "import torch.nn.functional as F  # functional conv helpers\n",
    "from IPython import get_ipython  # notebook runtime access\n",
    "plt.style.use(\"seaborn-v0_8\")  # consistent styling\n",
    "get_ipython().run_line_magic(\"config\", \"InlineBackend.figure_format = 'retina'\")  # crisp notebook figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef117c",
   "metadata": {},
   "source": [
    "## Convolution (toy example)\n",
    "\n",
    "Build a tiny synthetic example: a white square inside a dark canvas filtered with a cross-shaped kernel.\n",
    "The output highlights edges because the kernel sums neighbours and subtracts the centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(1, 1, 6, 6)  # blank canvas\n",
    "x[:, :, 2:4, 2:4] = 1.0  # add bright 2x2 square in the centre\n",
    "kernel = torch.tensor([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]]).view(1, 1, 3, 3)  # laplacian-style filter\n",
    "y = F.conv2d(x, kernel, stride=1, padding=0)  # valid convolution\n",
    "print(\"input shape:\", tuple(x.shape))  # confirm input shape\n",
    "print(\"output shape:\", tuple(y.shape))  # confirm output shape\n",
    "print(\"output map:\\n\", y.squeeze())  # display response values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081f3cb",
   "metadata": {},
   "source": [
    "## Padding vs stride (shapes)\n",
    "\n",
    "Demonstrate how padding preserves spatial size while stride downsamples feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93009674",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.randn(1, 1, 5, 5)  # random 5x5 input\n",
    "kernel = torch.ones(1, 1, 3, 3)  # 3x3 averaging kernel\n",
    "no_pad = F.conv2d(sample, kernel, stride=1, padding=0)  # valid conv\n",
    "same_pad = F.conv2d(sample, kernel, stride=1, padding=1)  # keep size\n",
    "stride_two = F.conv2d(sample, kernel, stride=2, padding=0)  # stride-2 conv\n",
    "print(\"no padding:\", tuple(no_pad.shape[-2:]))  # show spatial dims\n",
    "print(\"padding=1:\", tuple(same_pad.shape[-2:]))  # show same-size dims\n",
    "print(\"stride=2:\", tuple(stride_two.shape[-2:]))  # show downsampled dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400ce21",
   "metadata": {},
   "source": [
    "## Tiny CNN (shape tracing)\n",
    "\n",
    "Stack two conv/pool blocks and confirm intermediate shapes along the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e010461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  # neural network modules\n",
    "class TinyCNN(nn.Module):  # simple two-block CNN\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(  # feature extractor\n",
    "            nn.Conv2d(1, 8, 3, padding=1),  # conv block 1 keeps resolution\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.MaxPool2d(2),  # downsample by 2\n",
    "            nn.Conv2d(8, 16, 3, padding=1),  # conv block 2\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.MaxPool2d(2),  # downsample by 2 again\n",
    "        )\n",
    "        self.classifier = nn.Sequential(  # linear head\n",
    "            nn.Flatten(),  # flatten to (B, features)\n",
    "            nn.Linear(16 * 7 * 7, 10),  # map to logits\n",
    "        )\n",
    "    def forward(self, x):  # forward propagation\n",
    "        x = self.features(x)  # apply conv blocks\n",
    "        return self.classifier(x)  # project to class scores\n",
    "model = TinyCNN()  # instantiate network\n",
    "sample = torch.randn(4, 1, 28, 28)  # mini-batch of images\n",
    "feats = model.features(sample)  # extract feature maps\n",
    "logits = model(sample)  # run full network\n",
    "print(\"input:\", tuple(sample.shape))  # show input shape\n",
    "print(\"features:\", tuple(feats.shape))  # show feature map shape\n",
    "print(\"logits:\", tuple(logits.shape))  # show classifier output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f50625",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Compute output shapes layer-by-layer for a CNN; verify with a forward pass.\n",
    "2. Visualize feature maps for one input and describe qualitative differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dadd10",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
