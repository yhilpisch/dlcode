{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26068a15",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11754972",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ff35d",
   "metadata": {},
   "source": [
    "# Chapter 13 — Sequences and Language Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73a716",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This Colab‑ready notebook mirrors Chapter 13 and walks through:\n",
    "- Tokenization + padding/mask\n",
    "- Embedding lookup with simple sinusoidal positions\n",
    "- A mean‑embedding baseline and a GRU classifier\n",
    "- A 3D toy embedding space for intuition\n",
    "\n",
    "Run cells top‑to‑bottom; keep seeds fixed when comparing variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cb663",
   "metadata": {},
   "source": [
    "## Tokenization and padding\n",
    "\n",
    "Turn raw strings into token IDs and pad to a common length. We also build a mask (1 on tokens, 0 on pads) for use in pooling/attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal vocabulary for demonstration (pad has ID 0)  #  build lookup\n",
    "vocab = {'<pad>':0, 'good':1, 'bad':2, 'movie':3, 'is':4, 'indeed':5, 'great':6}\n",
    "\n",
    "import torch  #  tensor library\n",
    "from torch.nn.utils.rnn import pad_sequence  #  pad variable-length sequences\n",
    "\n",
    "def encode(text: str, vocab: dict) -> torch.Tensor:\n",
    "    \"\"\"Turn a whitespace string into token IDs.\"\"\"\n",
    "    ids = [vocab.get(tok, vocab['<pad>']) for tok in text.split()]  #  OOV -> <pad> for simplicity\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def encode_batch(texts, vocab):\n",
    "    \"\"\"Return (ids, lengths, mask) for a list of texts.\"\"\"\n",
    "    seqs = [encode(t, vocab) for t in texts]  #  list of 1D tensors\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)  #  original lengths\n",
    "    ids = pad_sequence(seqs, batch_first=True, padding_value=vocab['<pad>'])  #  (B, T_max)\n",
    "    mask = (ids != vocab['<pad>']).long()  #  1 on tokens, 0 on pads\n",
    "    return ids, lengths, mask\n",
    "\n",
    "texts = [\"this movie is indeed great\", \"good movie\", \"bad movie\"]\n",
    "ids, lengths, mask = encode_batch(texts, vocab)\n",
    "print(ids)      #  (B, T_max)\n",
    "print(lengths)  #  originals\n",
    "print(mask[0])  #  mask for sample 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c25d2",
   "metadata": {},
   "source": [
    "## Embedding lookup\n",
    "\n",
    "Combine learned token embeddings with simple sinusoidal positions to create (B, T, D) representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "embed = nn.Embedding(num_embeddings=len(vocab), embedding_dim=8)  #  tiny dim for demo\n",
    "ids, lengths, _ = encode_batch([\"good movie\", \"bad movie\"], vocab)  #  batch of IDs\n",
    "pos = torch.arange(ids.size(1)).unsqueeze(0)  #  (1, T)\n",
    "\n",
    "def sinusoidal_positions(pos: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    i = torch.arange(dim)\n",
    "    angle = pos.unsqueeze(-1) / (10000 ** (2 * (i//2) / dim))\n",
    "    enc = torch.zeros(pos.size(0), pos.size(1), dim)\n",
    "    enc[..., 0::2] = torch.sin(angle[..., 0::2])\n",
    "    enc[..., 1::2] = torch.cos(angle[..., 1::2])\n",
    "    return enc\n",
    "\n",
    "pos_enc = sinusoidal_positions(pos, dim=8)  #  (1, T, D)\n",
    "emb = embed(ids) + pos_enc  #  token + position -> (B, T, D)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa26f19",
   "metadata": {},
   "source": [
    "## Tiny sentiment toy (mean embedding)\n",
    "\n",
    "A fast baseline: average embeddings over time and feed to a linear head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class MeanEmbeddingClassifier(nn.Module):  #  baseline without RNN\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dim)\n",
    "        self.fc = nn.Linear(dim, 2)  #  binary\n",
    "    def forward(self, ids):\n",
    "        x = self.embed(ids).mean(dim=1)  #  mean over time\n",
    "        return self.fc(x)\n",
    "\n",
    "model = MeanEmbeddingClassifier(len(vocab), 16)\n",
    "ids, _, _ = encode_batch([\"good movie\", \"bad movie\"], vocab)\n",
    "probs = model(ids).softmax(-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518d579",
   "metadata": {},
   "source": [
    "## GRU classifier (packed sequences)\n",
    "\n",
    "Pack padded sequences to avoid computing on pad tokens and classify with the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcda67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, hidden=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, ids, lengths):\n",
    "        x = self.embed(ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.gru(packed)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "texts = [\"good movie\", \"bad movie\", \"good good movie\"]\n",
    "ids, lengths, _ = encode_batch(texts, vocab)\n",
    "probs = GRUClassifier(len(vocab))(ids, lengths).softmax(-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4d11",
   "metadata": {},
   "source": [
    "## 3D Toy embedding space (visual check)\n",
    "\n",
    "A quick inline plot to see clusters in 3D; no file writes needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdaaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "\n",
    "points = {\n",
    "    'good': (1.6, 1.1, 0.9),\n",
    "    'great': (1.7, 1.0, 1.1),\n",
    "    'bad': (-1.6, -1.1, -0.9),\n",
    "    'awful': (-1.7, -0.9, -1.1),\n",
    "    'movie': (0.2, 0.4, 0.15),\n",
    "    'film': (0.25, 0.45, 0.25),\n",
    "}\n",
    "colors = {'good':'tab:green','great':'tab:green','bad':'tab:red','awful':'tab:red','movie':'tab:blue','film':'tab:blue'}\n",
    "\n",
    "fig = plt.figure(figsize=(5.2, 3.6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for w,(x,y,z) in points.items():\n",
    "    ax.scatter([x],[y],[z], c=colors[w], s=50)\n",
    "    dz = 0.10 if w in {'good','awful','movie'} else -0.12\n",
    "    va = 'bottom' if dz>0 else 'top'\n",
    "    ax.text(x, y, z+dz, w, ha='center', va=va)\n",
    "ax.set_box_aspect((1.2,1.0,0.9))\n",
    "ax.view_init(elev=24, azim=-35)\n",
    "ax.grid(True, alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f4ba97",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
