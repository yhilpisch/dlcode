{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940fd5f9",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f2ab8",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "\n",
    "**Dr. Yves J. Hilpisch with GPT-5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823cf43",
   "metadata": {},
   "source": [
    "# Chapter 1 — Introduction to Machine Learning\n",
    "\n",
    "This Colab-ready notebook mirrors the minimal regression example and basic workflow from the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n\n",
    "This notebook provides a concise, hands-on walkthrough of Deep Learning Basics with PyTorch.\n",
    "Use it as a companion to the chapter: run each cell, read the short notes,\n",
    "and try small variations to build intuition.\n\n",
    "Tips:\n",
    "- Run cells top to bottom; restart kernel if state gets confusing.\n",
    "- Prefer small, fast experiments; iterate quickly and observe outputs.\n",
    "- Keep an eye on shapes, dtypes, and devices when using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4ba1",
   "metadata": {},
   "source": [
    "## Highlights\n",
    "\n",
    "- Follow the data → features → model → metrics loop from the chapter on a dataset you can reason about.\n",
    "- Fit a transparent linear regression and inspect parameters before looking at any plots.\n",
    "- Use visuals and metrics to question whether the model behaves as expected.\n",
    "\n",
    "## Guidance\n",
    "\n",
    "1. Run the imports (and optionally your environment checks) to make sure packages are available.\n",
    "2. Fit the tiny regression model, inspect coefficients, and compare predictions with the targets.\n",
    "3. Plot the fitted line to match the chapter's visual intuition.\n",
    "4. Practice the evaluation step with a simple train/test split and MAE.\n",
    "\n",
    "## Explanation\n",
    "\n",
    "This notebook mirrors Chapter 1 so you can rehearse the workflow hands-on. Treat it as a sandbox: verify imports, fit the toy model, and note how the metric reflects the tiny noise we inject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a4031",
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Optional: ensure packages are present (Colab usually has these)\n",
    "# !pip -q install scikit-learn matplotlib numpy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8') # plotting  # plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21e7ee",
   "metadata": {},
   "source": [
    "## Minimal Linear Regression\n",
    "\n",
    "We handcraft four points so you can predict the outcome before running code. Expect a slope near 1.0 and an intercept close to zero—perfect for tracing the workflow manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.0], [1.0], [2.0], [3.0]], dtype = float) # inputs  # inputs\n",
    "y = np.array([0.0, 1.0, 2.1, 2.9], dtype = float) # targets  # targets/labels\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "coef = model.coef_.ravel().tolist()\n",
    "intercept = float(model.intercept_)\n",
    "\n",
    "print(f\"coef = {coef} intercept = {intercept:.3f}\")\n",
    "\n",
    "pred = model.predict(X)\n",
    "for xi, yi, pi in zip(X.ravel(), y, pred):\n",
    "    print(f\"x = {xi:.1f} y = {yi:.2f} pred = {pi:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 3))  # plotting\n",
    "plt.scatter(X, y, label = 'data')  # plotting\n",
    "xx = np.linspace(0, 3, 50).reshape(-1, 1)\n",
    "plt.plot(xx, model.predict(xx), 'r-', label = 'fit')  # plotting\n",
    "plt.xlabel('x')  # plotting\n",
    "plt.ylabel('y')  # plotting\n",
    "plt.legend()  # plotting\n",
    "plt.tight_layout()  # plotting\n",
    "plt.show()  # plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b67969",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- The learned slope and intercept match what you would sketch by hand, confirming the model mirrors your intuition.\n",
    "- Printing predictions alongside targets is the chapter's \"taste the sauce\" step—verify the workflow before scaling up.\n",
    "- The plot should resemble Figure 1; if it doesn't, revisit the data preparation and fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad14ba6",
   "metadata": {},
   "source": [
    "## Train/Test Split and MAE\n",
    "\n",
    "The chapter's workflow ends with evaluation. We add a touch of noise, split the data, and read mean absolute error (MAE) on the hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0) # reproducible noise\n",
    "X = np.linspace(0, 4, 20, dtype = float).reshape(-1, 1)  # inputs\n",
    "noise = rng.normal(0, 0.05, size = X.shape[0])\n",
    "y = 0.95 * X.ravel() + 0.1 + noise  # targets/labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.25, random_state = 42\n",
    ")\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33689b12",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- `train_test_split` with a fixed `random_state` keeps experiments reproducible.\n",
    "- MAE reports average error in the original units; expect a small value because the synthetic noise is mild.\n",
    "- Adjust the noise or `test_size` to practice how evaluation metrics react to data changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97099807",
   "metadata": {},
   "source": [
    "## Guidance: Environment Checks\n",
    "\n",
    "Before moving on, follow Section 1.5 from the book: run `python -m code.env_check`, `python code/hello_world.py`, and `python code/ch01/minimal_regression_sklearn.py` to verify your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24717659",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n\n",
    "1. Fit a linear model to a different synthetic dataset; visualize residuals.\n",
    "2. Add one feature and observe how metrics change (good and bad cases).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ch01_intro_ml.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
